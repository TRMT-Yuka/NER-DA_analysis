{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1348dfe6",
   "metadata": {},
   "source": [
    "# 結果の集計"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87835cb",
   "metadata": {},
   "source": [
    "## ラベルごと 21/11/28 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f755534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404957a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for n in [5,7,10,15,20]:\n",
    "# # for n in [3]:\n",
    "# for data_name in [\"1-167th\",\"2-167th\",\"3-167th\"]:\n",
    "#     for task_name in [\"f_small\",\"Simple_BI_small\",\"Simple_O_small\",\"TreePos_BI_small\",\"TreePos_O_small\"]:\n",
    "#             for n in [3]:\n",
    "#             print(n,\" \",task_name)\n",
    "\n",
    "#             references = []\n",
    "#             with open(\"data/f/test.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "#                 for line in f:\n",
    "#                     references.append(json.loads(line)[\"tags\"])\n",
    "\n",
    "#             predictions = []\n",
    "#             with open(\"result/\"+data_name+\"/\"+task_name+\"/\"+str(n)+\"/predictions.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "#                 for line in f:\n",
    "#                     predictions.append(line.replace(\"\\n\",\"\").split(\" \"))\n",
    "\n",
    "#             label_set = set()\n",
    "#             for l in references+predictions:\n",
    "#                 for label in list(set(l)):\n",
    "#                     label_set.add(label)\n",
    "\n",
    "#             label_set.remove(\"O\")\n",
    "#             label_d = dict()\n",
    "#             for k in label_set:\n",
    "#                 label_d[k] = \"L\"+k.replace(\"/\",\"_\").replace(\"-\",\"_\")\n",
    "\n",
    "#             for l in references:\n",
    "#                 for i in range(len(l)):\n",
    "#                     if l[i] in label_d:\n",
    "#                         l[i] = label_d[l[i]]\n",
    "\n",
    "#             for l in predictions:\n",
    "#                 for i in range(len(l)):\n",
    "#                     if l[i] in label_d:\n",
    "#                         l[i] = label_d[l[i]]\n",
    "\n",
    "#             results = seqeval.compute(predictions=predictions, references=references)\n",
    "#             print(1)\n",
    "#             results_overall = {}\n",
    "#             results_overall['overall_precision']=results['overall_precision']\n",
    "#             results_overall['overall_recall']=results['overall_recall']\n",
    "#             results_overall['overall_f1']=results['overall_f1']\n",
    "#             results_overall['overall_accuracy']=results['overall_accuracy']\n",
    "\n",
    "#             del results['overall_precision']\n",
    "#             del results['overall_recall']\n",
    "#             del results['overall_f1']\n",
    "#             del results['overall_accuracy']#項目に含まれる数値のみを値とするkeyを削除\n",
    "\n",
    "#             with open(\"result/all_label_result/\"+task_name+\"_overall.csv\", 'w') as csvfile:\n",
    "#                 writer = csv.DictWriter(csvfile, fieldnames=[\"overall_precision\",\"overall_recall\",\"overall_f1\",\"overall_accuracy\"],lineterminator='\\n')\n",
    "#                 writer.writeheader()\n",
    "#                 writer.writerow(results_overall)\n",
    "\n",
    "#             print(2)\n",
    "#             result_csv = []\n",
    "#             for k in results.keys():\n",
    "#                 results[k][\"label\"] = k\n",
    "#                 result_csv.append(results[k])\n",
    "\n",
    "#             with open(\"result/all_label_result/epoch\"+str(n)+\"_\"+task_name+\"_each_label.csv\", 'w') as csvfile:\n",
    "#                 writer = csv.DictWriter(csvfile, fieldnames=[\"label\",\"precision\",\"recall\",\"f1\",\"number\"],lineterminator='\\n')\n",
    "#                 writer.writeheader()\n",
    "#                 writer.writerows([results[data] for data in results])\n",
    "\n",
    "#         !rundll32 user32.dll,MessageBeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f07525",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_name in [\"3-167th\"]:\n",
    "    \n",
    "    for task_name in [\"Simple_ALL_small\",\"TreePos_ALL_small\"]:\n",
    "        print(task_name)\n",
    "\n",
    "        references = []\n",
    "        with open(\"data/f/test.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                references.append(json.loads(line)[\"tags\"])\n",
    "\n",
    "        predictions = []\n",
    "        with open(\"result/\"+data_name+\"/\"+task_name+\"/predictions.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                predictions.append(line.replace(\"\\n\",\"\").split(\" \"))\n",
    "\n",
    "        label_set = set()\n",
    "        for l in references+predictions:\n",
    "            for label in list(set(l)):\n",
    "                label_set.add(label)\n",
    "\n",
    "        label_set.remove(\"O\")\n",
    "        label_d = dict()\n",
    "        for k in label_set:\n",
    "            label_d[k] = \"L\"+k.replace(\"/\",\"_\").replace(\"-\",\"_\")\n",
    "\n",
    "        for l in references:\n",
    "            for i in range(len(l)):\n",
    "                if l[i] in label_d:\n",
    "                    l[i] = label_d[l[i]]\n",
    "\n",
    "        for l in predictions:\n",
    "            for i in range(len(l)):\n",
    "                if l[i] in label_d:\n",
    "                    l[i] = label_d[l[i]]\n",
    "\n",
    "        results = seqeval.compute(predictions=predictions, references=references)\n",
    "        print(1)\n",
    "        results_overall = {}\n",
    "        results_overall['overall_precision']=results['overall_precision']\n",
    "        results_overall['overall_recall']=results['overall_recall']\n",
    "        results_overall['overall_f1']=results['overall_f1']\n",
    "        results_overall['overall_accuracy']=results['overall_accuracy']\n",
    "\n",
    "        del results['overall_precision']\n",
    "        del results['overall_recall']\n",
    "        del results['overall_f1']\n",
    "        del results['overall_accuracy']#項目に含まれる数値のみを値とするkeyを削除\n",
    "\n",
    "        with open(\"result/all_label_result/\"+task_name+\"_overall.csv\", 'w') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=[\"overall_precision\",\"overall_recall\",\"overall_f1\",\"overall_accuracy\"],lineterminator='\\n')\n",
    "            writer.writeheader()\n",
    "            writer.writerow(results_overall)\n",
    "\n",
    "        print(2)\n",
    "        result_csv = []\n",
    "        for k in results.keys():\n",
    "            results[k][\"label\"] = k\n",
    "            result_csv.append(results[k])\n",
    "\n",
    "        with open(\"result/all_label_result/\"+task_name+\"_each_label.csv\", 'w') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=[\"label\",\"precision\",\"recall\",\"f1\",\"number\"],lineterminator='\\n')\n",
    "            writer.writeheader()\n",
    "            writer.writerows([results[data] for data in results])\n",
    "\n",
    "    !rundll32 user32.dll,MessageBeep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e0a933",
   "metadata": {},
   "source": [
    "## モデル予想まとめ 　22/3/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efbb4761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_name=\"3-167th\"\n",
    "n = 3\n",
    "pre_s = {}\n",
    "\n",
    "ref = []\n",
    "tokens = []\n",
    "with open(\"data/f/test.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        ref.append(json.loads(line)[\"tags\"])\n",
    "        tokens.append(json.loads(line)[\"tokens\"])\n",
    "pre_s[\"tokens\"] = tokens\n",
    "pre_s[\"ans\"] = ref\n",
    "\n",
    "for task_name in [\"f_small\",\"Simple_BI_small\",\"Simple_O_small\",\"TreePos_BI_small\",\"TreePos_O_small\",\"Simple_ALL_small\",\"TreePos_ALL_small\"]:\n",
    "    pre = []\n",
    "    with open(\"result/\"+data_name+\"/\"+task_name+\"/\"+str(n)+\"/predictions.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            pre.append(line.replace(\"\\n\",\"\").split(\" \"))\n",
    "    pre_s[task_name] = pre\n",
    "    \n",
    "pre_lines = []\n",
    "for i in range(len(pre_s[\"ans\"])):\n",
    "    new_d = {}\n",
    "    for k in [\"tokens\",\"ans\",\"f_small\",\"Simple_BI_small\",\"Simple_O_small\",\"TreePos_BI_small\",\"TreePos_O_small\",\"Simple_ALL_small\",\"TreePos_ALL_small\"]:\n",
    "        new_d[k]=pre_s[k][i]\n",
    "    pre_lines.append(new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53c4e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = set()\n",
    "with open(\"data/labels.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        label_set.add(line.replace(\"\\n\",\"\"))\n",
    "label_set.remove(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c9a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_label_set = {\"location-GPE\"}#試運転用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d875a203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product-train\n",
      "building-other\n",
      "organization-company\n",
      "event-other\n",
      "location-island\n",
      "art-painting\n",
      "organization-sportsleague\n",
      "other-law\n",
      "other-astronomything\n",
      "other-god\n",
      "location-road/railway/highway/transit\n",
      "person-soldier\n",
      "person-scholar\n",
      "product-software\n",
      "building-hotel\n",
      "organization-media/newspaper\n",
      "event-sportsevent\n",
      "person-athlete\n",
      "person-actor\n",
      "person-director\n",
      "organization-sportsteam\n",
      "organization-education\n",
      "building-hospital\n",
      "event-attack/battle/war/militaryconflict\n",
      "building-library\n",
      "other-award\n",
      "location-park\n",
      "other-biologything\n",
      "person-politician\n",
      "other-chemicalthing\n",
      "building-restaurant\n",
      "product-weapon\n",
      "product-ship\n",
      "organization-politicalparty\n",
      "organization-showorganization\n",
      "building-theater\n",
      "product-airplane\n",
      "building-airport\n",
      "location-GPE\n",
      "location-bodiesofwater\n",
      "other-language\n",
      "location-mountain\n",
      "other-currency\n",
      "event-election\n",
      "location-other\n",
      "person-other\n",
      "product-food\n",
      "product-other\n",
      "art-music\n",
      "art-film\n",
      "event-disaster\n",
      "product-car\n",
      "other-livingthing\n",
      "other-medical\n",
      "person-artist/author\n",
      "organization-other\n",
      "event-protest\n",
      "art-broadcastprogram\n",
      "other-educationaldegree\n",
      "organization-religion\n",
      "building-sportsfacility\n",
      "art-writtenart\n",
      "product-game\n",
      "other-disease\n",
      "art-other\n",
      "organization-government/governmentagency\n"
     ]
    }
   ],
   "source": [
    "for label in label_set:\n",
    "    print(label)\n",
    "    all_txt = \"\"\n",
    "    with open(\"result/3-167th/pre/\"+label.replace(\"/\",\"_\")+\".html\", mode='w',encoding='utf-8') as f:\n",
    "        for d in pre_lines:\n",
    "            ans = \"ans    :\"\n",
    "            small = \"small   :\"\n",
    "            MR = \"MR      :\"   \n",
    "            MR_pos = \"MR_pos     :\"\n",
    "            oLwRT = \"oLwRT     :\"\n",
    "            oLwRT_pos = \"oLwRT_pos  :\"\n",
    "            MR_oLwRT = \"MR_oLwRT   :\"\n",
    "            MR_oLwRT_pos = \"MR_oLwRT_pos:\"\n",
    "            if label in d[\"ans\"]:\n",
    "                if d[\"ans\"]!=d[\"f_small\"] or d[\"ans\"]!=d[\"Simple_BI_small\"] or d[\"ans\"]!=d[\"TreePos_BI_small\"] or d[\"ans\"]!=d[\"Simple_O_small\"] or d[\"ans\"]!=d[\"TreePos_O_small\"] or d[\"ans\"]!=d[\"Simple_ALL_small\"] or d[\"ans\"]!=d[\"TreePos_ALL_small\"]:\n",
    "                    for i, t in enumerate(d[\"tokens\"]):\n",
    "\n",
    "                        if d[\"ans\"][i] != \"O\":\n",
    "                            ans += \"<font color='blue'>\"+t+\"</font> \"\n",
    "                        else:\n",
    "                            ans += t+\" \"\n",
    "\n",
    "                        if d[\"f_small\"][i] != \"O\":\n",
    "                            small += \"<font color='blue'>\"+t+\"</font> \"\n",
    "                        else:\n",
    "                            small += t+\" \"\n",
    "\n",
    "                        if d[\"Simple_BI_small\"][i] != \"O\":\n",
    "                            MR += \"<font color='blue'>\"+t+\"</font> \"\n",
    "                        else:\n",
    "                            MR += t+\" \"\n",
    "\n",
    "                        if d[\"TreePos_BI_small\"][i] != \"O\":\n",
    "                            MR_pos += \"<font color='blue'>\"+t+\"</font> \"\n",
    "                        else:\n",
    "                            MR_pos += t+\" \"\n",
    "\n",
    "                        if d[\"Simple_O_small\"][i] != \"O\":\n",
    "                            oLwRT += \"<font color='blue'>\"+t+\"</font> \"\n",
    "                        else:\n",
    "                            oLwRT += t+\" \"\n",
    "\n",
    "                        if d[\"TreePos_O_small\"][i] != \"O\":\n",
    "                            oLwRT_pos += \"<font color='blue'>\"+t+\"</font> \"\n",
    "                        else:\n",
    "                            oLwRT_pos += t+\" \"\n",
    "\n",
    "                        if d[\"Simple_ALL_small\"][i] != \"O\":\n",
    "                            MR_oLwRT += \"<font color='blue'>\"+t+\"</font> \"\n",
    "                        else:\n",
    "                            MR_oLwRT += t+\" \"\n",
    "\n",
    "                        if d[\"TreePos_ALL_small\"][i] != \"O\":\n",
    "                            MR_oLwRT_pos += \"<font color='blue'>\"+t+\"</font> \"\n",
    "                        else:\n",
    "                            MR_oLwRT_pos += t+\" \"\n",
    "\n",
    "                    f.write(\" \".join(d[\"ans\"])+\"<br>\"+\" \".join(d[\"f_small\"])+\"<br>\"+\" \".join(d[\"Simple_BI_small\"])+\"<br>\"+\" \".join(d[\"TreePos_BI_small\"])+\"<br>\"+\" \".join(d[\"Simple_O_small\"])+\"<br>\"+\" \".join(d[\"TreePos_O_small\"])+\"<br>\"+\" \".join(d[\"Simple_ALL_small\"])+\"<br>\"+\" \".join(d[\"TreePos_ALL_small\"])+\"<br><br>\")\n",
    "                    f.write(ans+\"<br>\"+small+\"<br>\"+MR+\"<br>\"+MR_pos+\"<br>\"+oLwRT+\"<br>\"+oLwRT_pos+\"<br>\"+MR_oLwRT+\"<br>\"+MR_oLwRT_pos+\"<br><br>\")\n",
    "\n",
    "# for label in test_label_set:\n",
    "#     all_txt = \"\"\n",
    "#     with open(\"result/html/\"+label+\".html\", mode='w',encoding='utf-8') as f:\n",
    "#         for d in pre_lines:\n",
    "#             ans = \"ans    :\"\n",
    "#             small = \"small   :\"\n",
    "#             MR = \"MR      :\"   \n",
    "#             MR_pos = \"MR_pos     :\"\n",
    "#             oLwRT = \"oLwRT     :\"\n",
    "#             oLwRT_pos = \"oLwRT_pos  :\"\n",
    "#             MR_oLwRT = \"MR_oLwRT   :\"\n",
    "#             MR_oLwRT_pos = \"MR_oLwRT_pos:\"\n",
    "#             if label in d[\"ans\"]:\n",
    "#                 for i, t in enumerate(d[\"tokens\"]):\n",
    "                    \n",
    "#                     if d[\"ans\"][i] != \"O\":\n",
    "#                         ans += \"【\"+d[\"ans\"][i]+\"】\"+\"<font color='blue'>\"+t+\"</font> \"\n",
    "#                     else:\n",
    "#                         ans += t+\" \"\n",
    "\n",
    "#                     if d[\"f_small\"][i] != \"O\":\n",
    "#                         small += \"【\"+d[\"ans\"][i]+\"】\"+\"<font color='blue'>\"+t+\"</font> \"\n",
    "#                     else:\n",
    "#                         small += t+\" \"\n",
    "\n",
    "#                     if d[\"Simple_BI_small\"][i] != \"O\":\n",
    "#                         MR += \"【\"+d[\"ans\"][i]+\"】\"+\"<font color='blue'>\"+t+\"</font> \"\n",
    "#                     else:\n",
    "#                         MR += t+\" \"\n",
    "\n",
    "#                     if d[\"TreePos_BI_small\"][i] != \"O\":\n",
    "#                         MR_pos += \"【\"+d[\"ans\"][i]+\"】\"+\"<font color='blue'>\"+t+\"</font> \"\n",
    "#                     else:\n",
    "#                         MR_pos += t+\" \"\n",
    "\n",
    "#                     if d[\"Simple_O_small\"][i] != \"O\":\n",
    "#                         oLwRT += \"【\"+d[\"ans\"][i]+\"】\"+\"<font color='blue'>\"+t+\"</font> \"\n",
    "#                     else:\n",
    "#                         oLwRT += t+\" \"\n",
    "                    \n",
    "#                     if d[\"TreePos_O_small\"][i] != \"O\":\n",
    "#                         oLwRT_pos += \"【\"+d[\"ans\"][i]+\"】\"+\"<font color='blue'>\"+t+\"</font> \"\n",
    "#                     else:\n",
    "#                         oLwRT_pos += t+\" \"\n",
    "\n",
    "#                     if d[\"Simple_ALL_small\"][i] != \"O\":\n",
    "#                         MR_oLwRT += \"【\"+d[\"ans\"][i]+\"】\"+\"<font color='blue'>\"+t+\"</font> \"\n",
    "#                     else:\n",
    "#                         MR_oLwRT += t+\" \"\n",
    "\n",
    "#                     if d[\"TreePos_ALL_small\"][i] != \"O\":\n",
    "#                         MR_oLwRT_pos += \"【\"+d[\"ans\"][i]+\"】\"+\"<font color='blue'>\"+t+\"</font> \"\n",
    "#                     else:\n",
    "#                         MR_oLwRT_pos += t+\" \"\n",
    "\n",
    "#                 f.write(ans+\"\\n\"+small+\"\\n\"+MR+\"\\n\"+MR_pos+\"\\n\"+oLwRT+\"\\n\"+oLwRT_pos+\"\\n\"+MR_oLwRT+\"\\n\"+MR_oLwRT_pos+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c85392",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "label_set = set()\n",
    "for l in references+predictions:\n",
    "    for label in list(set(l)):\n",
    "        label_set.add(label)\n",
    "\n",
    "label_set.remove(\"O\")\n",
    "    label_d = dict()\n",
    "    for k in label_set:\n",
    "        label_d[k] = \"L\"+k.replace(\"/\",\"_\").replace(\"-\",\"_\")\n",
    "\n",
    "    for l in references:\n",
    "        for i in range(len(l)):\n",
    "            if l[i] in label_d:\n",
    "                l[i] = label_d[l[i]]\n",
    "\n",
    "    for l in predictions:\n",
    "        for i in range(len(l)):\n",
    "            if l[i] in label_d:\n",
    "                l[i] = label_d[l[i]]\n",
    "\n",
    "    results = seqeval.compute(predictions=predictions, references=references)\n",
    "    print(1)\n",
    "    results_overall = {}\n",
    "    results_overall['overall_precision']=results['overall_precision']\n",
    "    results_overall['overall_recall']=results['overall_recall']\n",
    "    results_overall['overall_f1']=results['overall_f1']\n",
    "    results_overall['overall_accuracy']=results['overall_accuracy']\n",
    "\n",
    "    del results['overall_precision']\n",
    "    del results['overall_recall']\n",
    "    del results['overall_f1']\n",
    "    del results['overall_accuracy']#項目に含まれる数値のみを値とするkeyを削除\n",
    "\n",
    "    with open(\"result/all_label_result/\"+task_name+\"_overall.csv\", 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=[\"overall_precision\",\"overall_recall\",\"overall_f1\",\"overall_accuracy\"],lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        writer.writerow(results_overall)\n",
    "\n",
    "    print(2)\n",
    "    result_csv = []\n",
    "    for k in results.keys():\n",
    "        results[k][\"label\"] = k\n",
    "        result_csv.append(results[k])\n",
    "\n",
    "    with open(\"result/all_label_result/\"+task_name+\"_each_label.csv\", 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=[\"label\",\"precision\",\"recall\",\"f1\",\"number\"],lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        writer.writerows([results[data] for data in results])\n",
    "\n",
    "    !rundll32 user32.dll,MessageBeep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5647db7f",
   "metadata": {},
   "source": [
    "## 結果の代表値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da6088b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json\n",
    "import json\n",
    "\n",
    "all_results = []\n",
    "for n in [1,2,3]:\n",
    "    for method in [\"f_small\",\"Simple_BI_small\",\"Simple_O_small\",\"TreePos_BI_small\",\"TreePos_O_small\"]:\n",
    "        for e in [1,2,3,5,7,10]:\n",
    "            try:\n",
    "                with open(\"result/\"+str(n)+\"-167th/\"+method+\"/\"+str(e)+\"/all_results.json\",\"r\") as f:\n",
    "                        d = json.loads(f.read())\n",
    "                        d[\"scale\"] = n\n",
    "                        d[\"method\"] = method\n",
    "                        d[\"epoch\"] = e\n",
    "                        all_results.append(d)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7147231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"n-167th_results_full.csv\",\"w\",encoding=\"utf-8\",newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames = list(all_results[0].keys()))\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd1e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_select = []\n",
    "for d in all_results:\n",
    "    new_d = {}\n",
    "    new_d[\"scale\"] = str(d[\"scale\"])+\"/167\"\n",
    "    new_d[\"method\"] = d[\"method\"]\n",
    "    new_d[\"epoch\"] = d[\"epoch\"]\n",
    "    new_d[\"accuracy\"] = d[\"predict_overall_accuracy\"]\n",
    "    new_d[\"fi\"] = d[\"predict_overall_f1\"]\n",
    "    new_d[\"precision\"] = d[\"predict_overall_precision\"]\n",
    "    new_d[\"recall\"] = d[\"predict_overall_recall\"]\n",
    "    new_d[\"loss\"] = d[\"predict_loss\"]\n",
    "    all_results_select.append(new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96703535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"n-167th_results+12.csv\",\"w\",encoding=\"utf-8\",newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames = list(all_results_select[0].keys()))\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_results_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a54bbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scale': '1/167',\n",
       " 'method': 'f_small',\n",
       " 'epoch': 1,\n",
       " 'accuracy': 0.7997891545989703,\n",
       " 'fi': 0.13897894205731054,\n",
       " 'precision': 0.15862425036736963,\n",
       " 'recall': 0.12366346034760352,\n",
       " 'loss': 0.9562876224517822}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results_select[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0e6a60",
   "metadata": {},
   "source": [
    "# 各学習データそのものの集計 23/8/29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23e9ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = set()\n",
    "with open(\"data/labels.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        label_set.add(line.replace(\"\\n\",\"\"))\n",
    "label_set.remove(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c719746d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【2】9 / 10"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "all_agg_label_fTS = []\n",
    "            \n",
    "for n in range(3):\n",
    "    all_agg_label = []\n",
    "    for i in range(10):\n",
    "        print('\\r【%d】%d / %d' %(n,i,10), end='')\n",
    "        f_small_train = []\n",
    "\n",
    "        if n == 0:\n",
    "            filepass = \"data/f_small/train_3-167th_\"+str(i)+\".json\"\n",
    "        elif n == 1:\n",
    "            filepass = \"data/TreePos_All_small/3-167th_\"+str(i)+\".json\"\n",
    "        else:\n",
    "            filepass = \"data/SubTree/\"+str(i)+\"_x10.json\"\n",
    "\n",
    "        with open(filepass,\"r\") as f:\n",
    "            for line in f:\n",
    "                f_small_train.append(json.loads(line))\n",
    "\n",
    "        #とりあえず最初は出現回数は考えずトークン数の全種類を算出\n",
    "        #ただし合計トークン数を取得しておけば，ある程度種類の被りは出せるかも\n",
    "        agg_label = dict()\n",
    "\n",
    "        for label in label_set:\n",
    "            agg_label[label] = {\"label\":label,\"s_sum\":0,\"tokens\":set(),\"t_sum\":0}\n",
    "            for d in f_small_train:\n",
    "                if label in d[\"tags\"]:\n",
    "                    agg_label[label][\"s_sum\"] += 1\n",
    "\n",
    "                    for t in d[\"tokens\"]:\n",
    "                        agg_label[label][\"tokens\"].add(t)\n",
    "\n",
    "                    agg_label[label][\"t_sum\"] += len(d[\"tokens\"])\n",
    "\n",
    "        for k,v in agg_label.items():\n",
    "            all_agg_label.append(v)\n",
    "\n",
    "    #tokens削除        \n",
    "    all_agg_label_rm_tokens = []\n",
    "    for d in all_agg_label:\n",
    "        new_d = {'label':d[\"label\"],'s_sum':d[\"s_sum\"],'t_sum':d[\"t_sum\"]}\n",
    "        all_agg_label_rm_tokens.append(new_d)\n",
    "\n",
    "    #保存\n",
    "    output_filepass = [\"f_small_agg_label.csv\",\"TreePos_agg_label.csv\",\"SubTree_agg_label.csv\"]  \n",
    "    filedname_list = ['label','s_sum','t_sum']\n",
    "#     df = pd.io.json.json_normalize(all_agg_label_rm_tokens)\n",
    "#     df.to_csv(output_filepass)\n",
    "    with open(output_filepass[n],'w',encoding='utf-8',) as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames = filedname_list, delimiter = \",\",lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        for d in all_agg_label_rm_tokens:\n",
    "            writer.writerow(d)\n",
    "        \n",
    "    all_agg_label_fTS.append(all_agg_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c9ce6fb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-a1d5d9b4bf7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_agg_label_fTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "all_agg_label_fTS[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f671205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d in all_agg_label_fTS[0]:\n",
    "#     print(d[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f70882",
   "metadata": {},
   "source": [
    "## 結果集計コード　オリジナル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653563d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_full = []\n",
    "with open(\"data/Simple_BI_small/labeled_small_3-167th.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_full.append(json.loads(line))\n",
    "        \n",
    "with open(\"data/labels.txt\",\"r\") as f:\n",
    "    labels = [line.strip() for line in f]\n",
    "    \n",
    "labels_d_test = dict()\n",
    "for label in labels[1:]:\n",
    "    c = 0\n",
    "    for d in train_full:\n",
    "        if label in set(d[\"tags\"]):\n",
    "            c += 1\n",
    "    labels_d_test[label] = c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf25290",
   "metadata": {},
   "source": [
    "# 23/6/11  \n",
    "・10倍を集計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10706c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/labels.txt\",\"r\") as f:\n",
    "    labels = [line.strip() for line in f]\n",
    "    \n",
    "\n",
    "import json\n",
    "import csv\n",
    "path = \"result/10times/to50/\"\n",
    "for DA_method in [\"f\",\"Simple_ALL\",\"Simple_BI\",\"Simple_O\",\"TreePos_ALL\",\"TreePos_BI\",\"TreePos_O\"]:\n",
    "    the_method_result = []\n",
    "    for i in range(10):\n",
    "        with open(path+DA_method+\"_small/\"+str(i)+\"/all_results.json\",\"r\") as f:\n",
    "            json_dict = json.load(f)\n",
    "        the_method_result.append(json_dict)\n",
    "        \n",
    "    with open(path+DA_method+\"_10test.csv\",\"w\",encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames = json_dict, lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(the_method_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c334a5",
   "metadata": {},
   "source": [
    "# 23/9/5\n",
    "\n",
    "【本日のお品書き】  \n",
    "・otherの癒着問題を修正(完了)\n",
    "・結果を未学習のトークンとし再集計  \n",
    "・結果出力用ＨＴＭＬファイルの作成（完了）  \n",
    "　・各固有表現タグの学習データ  \n",
    "　・各固有表現タグのテストデータ（正解したもの）  \n",
    "　・各固有表現タグのテストデータ（不正解だったもの）  \n",
    "　・誤って該当固有表現に分類されたテストデータ  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937be828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(DA4ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45207090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "result = []\n",
    "for i in range(10):\n",
    "    print('\\r%d / %d' %(i, 9), end='')\n",
    "    result.append(DA4ner.manual_NER_eval(\"data/f/test.json\",\"result/SubTree/\"+str(i)+\"/predictions.txt\"))\n",
    "    \n",
    "import pandas as pd\n",
    "df = pd.io.json.json_normalize(result)\n",
    "df.to_csv('analysis/Subtree/manual_NRE_eval_0-9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10454262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#htmlの作成\n",
    "import json\n",
    "\n",
    "pre_s = {}\n",
    "ref = []\n",
    "tokens = []\n",
    "task_list = [\"f_small\",\"TreePos_All_small\",\"SubTree\"]\n",
    "\n",
    "with open(\"data/f/test.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        ref.append(json.loads(line)[\"tags\"])\n",
    "        tokens.append(json.loads(line)[\"tokens\"])\n",
    "pre_s[\"tokens\"] = tokens\n",
    "pre_s[\"ans\"] = ref\n",
    "\n",
    "for task_name in task_list:\n",
    "    pre = []\n",
    "    with open(\"analysis/FvsPOSALLvsSUBTREE/\"+task_name+\"/predictions.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            pre.append(line.replace(\"\\n\",\"\").split(\" \"))\n",
    "    pre_s[task_name] = pre\n",
    "    \n",
    "pre_lines = []\n",
    "for i in range(len(pre_s[\"ans\"])):\n",
    "    new_d = {}\n",
    "    for k in [\"tokens\",\"ans\"]+task_list:\n",
    "        new_d[k]=pre_s[k][i]\n",
    "    pre_lines.append(new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dfcc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = set()\n",
    "with open(\"data/labels.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        label_set.add(line.replace(\"\\n\",\"\"))\n",
    "label_set.remove(\"O\")\n",
    "\n",
    "for label in label_set:\n",
    "    print(label)\n",
    "    all_txt = \"\"\n",
    "    with open(\"analysis/SubTree_csv_html/html/\"+label.replace(\"/\",\"_\")+\".html\", mode='w',encoding='utf-8') as f:\n",
    "        for d in pre_lines:\n",
    "            ans = \"ans              :\"\n",
    "            small = \"small            :\"\n",
    "            TreePos_All_small = \"TreePos_ALL_small:\"\n",
    "            SubTree = \"SubTree          :\"\n",
    "            if label in d[\"ans\"]:\n",
    "                if d[\"ans\"]!=d[\"f_small\"] or d[\"ans\"]!=d[\"TreePos_All_small\"] or d[\"ans\"]!=d[\"SubTree\"]:\n",
    "                    for i, t in enumerate(d[\"tokens\"]):\n",
    "\n",
    "                        if d[\"ans\"][i] != \"O\":\n",
    "                            ans += \"<font color='blue'>\"+t+\"</font> \"\n",
    "                        else:\n",
    "                            ans += t+\" \"\n",
    "\n",
    "                        if d[\"f_small\"][i] != \"O\":\n",
    "                            small += \"<font color='blue'>\"+t+\"</font> \"\n",
    "                        else:\n",
    "                            small += t+\" \"\n",
    "\n",
    "                        if d[\"TreePos_All_small\"][i] != \"O\":\n",
    "                            TreePos_All_small += \"<font color='blue'>\"+t+\"</font> \"\n",
    "                        else:\n",
    "                            TreePos_All_small += t+\" \"\n",
    "\n",
    "                        if d[\"SubTree\"][i] != \"O\":\n",
    "                            SubTree += \"<font color='blue'>\"+t+\"</font> \"\n",
    "                        else:\n",
    "                            SubTree += t+\" \"\n",
    "                    f.write(\" \".join(d[\"ans\"])+\"<br>\"+\" \".join(d[\"f_small\"])+\"<br>\"+\" \".join(d[\"TreePos_All_small\"])+\"<br>\"+\" \".join(d[\"SubTree\"])+\"<br><br>\")\n",
    "                    f.write(ans+\"<br>\"+small+\"<br>\"+TreePos_All_small+\"<br>\"+SubTree+\"<br><br>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9959256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習に出現していないトークンを再集計\n",
    "label_set = set()\n",
    "with open(\"data/labels.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        label_set.add(line.replace(\"\\n\",\"\"))\n",
    "label_set.remove(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a1615f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ce8a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label in label_set:\n",
    "#     print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c459554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f_test = []\n",
    "with open(\"data/f/test.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        f_test.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c2bde8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agg_label = dict()\n",
    "for label in label_set:\n",
    "    test_agg_label[label] = {\"label\":label,\"s_sum\":0,\"tokens\":set(),\"t_sum\":0}\n",
    "    for d in f_test:\n",
    "        if label in d[\"tags\"]:\n",
    "            test_agg_label[label][\"s_sum\"] += 1\n",
    "            for t in d[\"tokens\"]:\n",
    "                test_agg_label[label][\"tokens\"].add(t)\n",
    "            test_agg_label[label][\"t_sum\"] += len(d[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35702d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_agg_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16c9a592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other-biologything\n"
     ]
    }
   ],
   "source": [
    "for k,v in test_agg_label.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "070d70e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【2】9 / 10"
     ]
    }
   ],
   "source": [
    "all_agg_label_fTS = []\n",
    "            \n",
    "for n in range(3):\n",
    "    all_agg_label = []\n",
    "    for i in range(10):\n",
    "        print('\\r【%d】%d / %d' %(n,i,10), end='')\n",
    "        f_small_train = []\n",
    "\n",
    "        if n == 0:\n",
    "            filepass = \"data/f_small/train_3-167th_\"+str(i)+\".json\"\n",
    "        elif n == 1:\n",
    "            filepass = \"data/TreePos_All_small/3-167th_\"+str(i)+\".json\"\n",
    "        else:\n",
    "            filepass = \"data/SubTree/\"+str(i)+\"_x10.json\"\n",
    "\n",
    "        with open(filepass,\"r\") as f:\n",
    "            for line in f:\n",
    "                f_small_train.append(json.loads(line))\n",
    "\n",
    "        #とりあえず最初は出現回数は考えずトークン数の全種類を算出\n",
    "        #ただし合計トークン数を取得しておけば，ある程度種類の被りは出せるかも\n",
    "        agg_label = dict()\n",
    "\n",
    "        for label in label_set:\n",
    "            agg_label[label] = {\"label\":label,\"s_sum\":0,\"tokens\":set(),\"t_sum\":0}\n",
    "            for d in f_small_train:\n",
    "                if label in d[\"tags\"]:\n",
    "                    agg_label[label][\"s_sum\"] += 1\n",
    "\n",
    "                    for t in d[\"tokens\"]:\n",
    "                        agg_label[label][\"tokens\"].add(t)\n",
    "\n",
    "                    agg_label[label][\"t_sum\"] += len(d[\"tokens\"])\n",
    "                    \n",
    "            agg_label[label][\"tt_tokens_num\"] = len(agg_label[label][\"tokens\"] & test_agg_label[label][\"tokens\"])\n",
    "            agg_label[label][\"only_train_tokens_num\"] = len(agg_label[label][\"tokens\"] - test_agg_label[label][\"tokens\"])\n",
    "            agg_label[label][\"only_test_tokens_num\"] = len(test_agg_label[label][\"tokens\"] - agg_label[label][\"tokens\"])\n",
    "            \n",
    "\n",
    "        for k,v in agg_label.items():\n",
    "            all_agg_label.append(v)\n",
    "\n",
    "    #tokens削除\n",
    "    all_agg_label_rm_tokens = []\n",
    "    for d in all_agg_label:\n",
    "        new_d = {'label':d[\"label\"],'s_sum':d[\"s_sum\"],'t_sum':d[\"t_sum\"],'tt_tokens_num':d[\"tt_tokens_num\"],'only_train_tokens_num':d[\"only_train_tokens_num\"],\"only_test_tokens_num\":d[\"only_test_tokens_num\"]}\n",
    "        all_agg_label_rm_tokens.append(new_d)\n",
    "\n",
    "    #保存\n",
    "    output_filepass = [\"f_small_agg_label.csv\",\"TreePos_agg_label.csv\",\"SubTree_agg_label.csv\"]  \n",
    "    filedname_list = ['label','s_sum','t_sum','tt_tokens_num',\"only_train_tokens_num\",\"only_test_tokens_num\"]\n",
    "#     df = pd.io.json.json_normalize(all_agg_label_rm_tokens)\n",
    "#     df.to_csv(output_filepass)\n",
    "    with open(output_filepass[n],'w',encoding='utf-8',) as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames = filedname_list, delimiter = \",\",lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        for d in all_agg_label_rm_tokens:\n",
    "            writer.writerow(d)\n",
    "        \n",
    "    all_agg_label_fTS.append(all_agg_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b896089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filedname_list = ['label','s_sum','t_sum','tt_tokens_num',\"only_train_tokens_num\",\"only_test_tokens_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bd9f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"0905.csv\",'w',encoding='utf-8',) as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = filedname_list, delimiter = \",\",lineterminator='\\n')\n",
    "    writer.writeheader()\n",
    "    for d in all_agg_label_set_fTS:\n",
    "        writer.writerow(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4d1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
