{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb870be",
   "metadata": {},
   "source": [
    "# ↓23/8/17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb138b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読込み用の関数\n",
    "def read_bin(filename):\n",
    "    with open(filename,'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def save_bin(data,filename):\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(data,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf1db9",
   "metadata": {},
   "source": [
    "### 構文木情報， tokenとtagのjson情報を読込"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cc449168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_trees = []\n",
    "for num in range(0, 131501, 500):\n",
    "    with open('data/Stanford_coreNLP/train_'+str(num)+'-'+str(num+499)+'.json_tree.binaryfile', 'rb') as f:\n",
    "        a_train_tree = pickle.load(f)\n",
    "        train_trees.extend(a_train_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "16a6ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train = []\n",
    "with open(\"data/f_id/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "    \n",
    "# id検索用の辞書作成\n",
    "idkey_train = {}\n",
    "for d in train:\n",
    "    idkey_train[d[\"id\"]] = d\n",
    "    \n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e17af",
   "metadata": {},
   "source": [
    "### 構文解析結果の処理に必要な関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from nltk.tree import Tree\n",
    "\n",
    "#部分木から具体的な単語を削除\n",
    "def remove_words(node):\n",
    "    for i, child in enumerate(node):\n",
    "        if isinstance(child, str):  # 葉ノード（単語）の場合\n",
    "            node[i] = ''  # 単語を空文字に置換\n",
    "        elif isinstance(child, Tree):\n",
    "            remove_words(child)\n",
    "\n",
    "# 各部分木、Treeタグ、トークンリスト、深さを取得\n",
    "def get_all_subtree_info(node, depth=0):\n",
    "    \n",
    "#     subtree用\n",
    "#     node_cp = copy.deepcopy(node)\n",
    "#     remove_words(node_cp)\n",
    "    subtree_tag = node.label()\n",
    "    subtree_tokens = [child for child in node.leaves()]\n",
    "    subtree_depth = depth\n",
    "\n",
    "    # 部分木の情報を辞書として格納\n",
    "    subtree_info = {\n",
    "#         'subtree': node_cp,\n",
    "        'tree_tag': subtree_tag,\n",
    "        'tokens': subtree_tokens,\n",
    "        'depth': subtree_depth\n",
    "    }\n",
    "    # 子ノードに対して再帰的に情報を収集\n",
    "    child_subtree_info = []\n",
    "    for child in node:\n",
    "        if isinstance(child, Tree):\n",
    "            child_subtree_info.extend(get_all_subtree_info(child, depth + 1))\n",
    "            \n",
    "    # 子ノードの情報を追加\n",
    "    subtree_info['children'] = child_subtree_info\n",
    "    return [subtree_info]\n",
    "\n",
    "\n",
    "# 全ての部分木情報をリスト内の対等な深さに格納\n",
    "def display_subtree_info(subtree_info, all_subtree):\n",
    "    new_d = copy.deepcopy(subtree_info)\n",
    "    \n",
    "    if \"children\" in new_d:\n",
    "        del new_d[\"children\"]\n",
    "     \n",
    "    all_subtree.append(new_d)\n",
    "    for child_info in subtree_info.get('children', []):\n",
    "        display_subtree_info(child_info,all_subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ff7275",
   "metadata": {},
   "source": [
    "### 部分木リストall_subtree（ner-tagを付ける前）を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "all_subtree = []\n",
    "n = len(train_trees)\n",
    "for i,tree in enumerate(train_trees):\n",
    "    print('\\r%d / %d' %(i, n), end='')\n",
    "    \n",
    "    # 全ての部分木情報を取得\n",
    "    all_subtree_info = get_all_subtree_info(tree)\n",
    "    subtree = []\n",
    "    \n",
    "    # 全ての部分木をリスト内の対等な深さに格納\n",
    "    for subtree_info in all_subtree_info:\n",
    "        display_subtree_info(subtree_info,subtree)\n",
    "    for d in subtree:\n",
    "        d[\"id\"] = i\n",
    "    all_subtree.extend(subtree)\n",
    "# subtreeを含む場合の具体的単語削除    \n",
    "# for d in all_subtree:\n",
    "#     remove_words(d[\"subtree\"])\n",
    "\n",
    "# 書き出し\n",
    "# with open(\"temp/all_subtree.binaryfile\", 'wb') as f:\n",
    "#     pickle.dump(all_subtree,f)\n",
    "\n",
    "# 読込み\n",
    "# with open(\"temp/all_subtree_rm_at.binaryfile\",'rb') as f:\n",
    "#     all_subtree = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b691f05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5849551 / 5849552"
     ]
    }
   ],
   "source": [
    "# all_subtreeのうち，交換対象にふさわしくないtree_tagを持つ部分木の削除\n",
    "　# 木全体であるROOT，'．''，'-LRB-','-RRB-'等tree_tag = tokenで種類がない物\n",
    "with open(\"temp/all_subtree.binaryfile\",'rb') as f:\n",
    "    all_subtree = pickle.load(f)\n",
    "\n",
    "remove_tag ={\"''\",',','-LRB-','-RRB-','.',':','``',\"ROOT\"}\n",
    "all_subtree_rm = []\n",
    "n = len(all_subtree)\n",
    "for i,d in enumerate(all_subtree):\n",
    "    print('\\r%d / %d' %(i, n), end='')\n",
    "    if d[\"tree_tag\"] not in remove_tag:\n",
    "        all_subtree_rm.append(d)\n",
    "        \n",
    "# with open(\"temp/all_subtree_rm.binaryfile\", 'wb') as f:\n",
    "#     pickle.dump(all_subtree_rm,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ddf7e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5310312 / 5310313"
     ]
    }
   ],
   "source": [
    "# all_subtree_rmのうち試験的に単一トークンの削除 (remove a token)\n",
    "all_subtree_rm_at = []\n",
    "\n",
    "n = len(all_subtree_rm)\n",
    "for i,d in enumerate(all_subtree_rm):\n",
    "    print('\\r%d / %d' %(i, n), end='')\n",
    "    if len(d[\"tokens\"]) != 1:\n",
    "        all_subtree_rm_at.append(d)\n",
    "\n",
    "#all_subtree_rm_atを格納\n",
    "# with open(\"temp/all_subtree_rm_at.binaryfile\", 'wb') as f:\n",
    "#     pickle.dump(all_subtree_rm_at,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f2b45c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読込み\n",
    "# with open(\"temp/all_subtree_rm_at.binaryfile\",'rb') as f:\n",
    "#     all_subtree_rm_at = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0ba70c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_subtree_rm_atのtokenである-LRB-と-RRB-を（）に戻す\n",
    "for d in all_subtree_rm_at:\n",
    "    new_tokens = []\n",
    "    for t in d[\"tokens\"]:\n",
    "        if t == \"-LRB-\":\n",
    "            new_tokens.append(\"(\")\n",
    "        elif t == \"-RRB-\":\n",
    "            new_tokens.append(\")\")\n",
    "        else:\n",
    "            new_tokens.append(t)\n",
    "    d[\"tokens\"] = new_tokens\n",
    "                \n",
    "\n",
    "#all_subtree_rm_at(exchang完了済)を格納\n",
    "# with open(\"temp/all_subtree_rm_at_exc.binaryfile\", 'wb') as f:\n",
    "#     pickle.dump(all_subtree_rm_at,f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "1051c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_subtree = all_subtree_rm_at\n",
    "# del all_subtree_rm\n",
    "# del all_subtree_rm_at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e032a237",
   "metadata": {},
   "source": [
    "### 部分木リストall_subtreeに対応するner-tag情報を付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "24076c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定部分tokenに対応するtag取得関数\n",
    "\n",
    "def get_tags_for_tokens(sub_tokens,d):\n",
    "    match_cnt = 0 #トークン列の連続が複数含まれていた場合の対処\n",
    "    sub_tags_and_index = dict()\n",
    "    n = len(d[\"tokens\"])\n",
    "    n_sub = len(sub_tokens)\n",
    "    \n",
    "    # d[\"token\"]列からsub_tokensの数の連続するリストを切り出しsub_tokensと比較\n",
    "    for i in range(0,n-n_sub+1):#同じトークン列の繰返し出現に備え,forは止めない\n",
    "        partof_d_tokens = d[\"tokens\"][i:i+n_sub]\n",
    "        if partof_d_tokens == sub_tokens:\n",
    "            sub_tags_and_index[\"sub_tags\"] = d[\"tags\"][i:i+n_sub]\n",
    "            sub_tags_and_index[\"start_index\"] = i\n",
    "            match_cnt += 1\n",
    "            \n",
    "    if match_cnt == 1:\n",
    "        return sub_tags_and_index\n",
    "    else:\n",
    "        return dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0e9d4a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# all_subtree=read_bin(\"temp/all_subtree_rm_at_exc.binaryfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "fd27a45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2082703 / 2082704"
     ]
    }
   ],
   "source": [
    "# 部分木のtag情報付与 更新版\n",
    "# need:idkey_train\n",
    "success_subtree = []#1854621\n",
    "fail_subtree = []#228083\n",
    "\n",
    "n = len(all_subtree)\n",
    "for i,sub_d in enumerate(all_subtree):#2082704\n",
    "    print('\\r%d / %d' %(i, n), end='')\n",
    "    try:\n",
    "        sub_tags_and_index = get_tags_for_tokens(sub_d[\"tokens\"],idkey_train[sub_d[\"id\"]])\n",
    "        \n",
    "        sub_d[\"tags\"] = sub_tags_and_index[\"sub_tags\"]\n",
    "        sub_d[\"start_index\"] = sub_tags_and_index[\"start_index\"]\n",
    "        success_subtree.append(sub_d)\n",
    "        \n",
    "    except Exception as e:\n",
    "        fail_subtree.append(sub_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "ff706b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_bin(success_subtree,\"temp/success_subtree.binaryfile\") \n",
    "# save_bin(fail_subtree,\"temp/fail_subtree.binaryfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17548245",
   "metadata": {},
   "source": [
    "### A - B をA-Bに戻す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "9ca660f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_subtree = read_bin(\"temp/success_subtree.binaryfile\")\n",
    "fail_subtree = read_bin(\"temp/fail_subtree.binaryfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "d6efbd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定部分tokenに対応するtag取得関数 A - BとA-Bの違い対応版\n",
    "\n",
    "def get_tags_for_diff_len_tokens(sub_tokens,d):\n",
    "    sub_tags_and_index = dict()\n",
    "    sub_s = \"\".join(sub_tokens)\n",
    "    stop_i_roop = 0\n",
    "    \n",
    "    for i in range(len(d[\"tokens\"])):\n",
    "        n = len(d[\"tokens\"][i])\n",
    "        if stop_i_roop == 1:\n",
    "            break\n",
    "            \n",
    "        if sub_s[0:n] == d[\"tokens\"][i]:\n",
    "            part_of_d = d[\"tokens\"][i]\n",
    "            end = 0\n",
    "            \n",
    "            for j in range(i+1,len(d[\"tokens\"])):\n",
    "                if d[\"tokens\"][j] in sub_s:\n",
    "                    part_of_d += d[\"tokens\"][j]\n",
    "                    if sub_s == part_of_d:\n",
    "                        end = j+1\n",
    "                        stop_i_roop = 1\n",
    "                        break\n",
    "    \n",
    "    sub_tags_and_index[\"sub_tokens\"] = d[\"tokens\"][i:j+1]\n",
    "    sub_tags_and_index[\"sub_tags\"] = d[\"tags\"][i:j+1]\n",
    "    sub_tags_and_index[\"start_index\"] = i\n",
    "    return sub_tags_and_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "fdd808c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228082 / 228083"
     ]
    }
   ],
   "source": [
    "fail_subtree_2 = []#5159\n",
    "\n",
    "n = len(fail_subtree)\n",
    "for i,sub_d in enumerate(fail_subtree):#228083\n",
    "    print('\\r%d / %d' %(i, n), end='')\n",
    "    try:\n",
    "        sub_tokens_tags_index = get_tags_for_diff_len_tokens(sub_d[\"tokens\"],idkey_train[sub_d[\"id\"]])\n",
    "        sub_d[\"tokens\"] = sub_tokens_tags_index[\"sub_tokens\"]\n",
    "        sub_d[\"tags\"] = sub_tokens_tags_index[\"sub_tags\"]\n",
    "        sub_d[\"start_index\"] = sub_tokens_tags_index[\"start_index\"]\n",
    "        success_subtree.append(sub_d)\n",
    "        \n",
    "    except Exception as e:\n",
    "        fail_subtree_2.append(sub_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "65675b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2077544 / 2077545"
     ]
    }
   ],
   "source": [
    "# success_subtreeのうち試験的に単一トークンの削除 (remove a token)\n",
    "success_subtree_at = []\n",
    "\n",
    "n = len(success_subtree)\n",
    "for i,d in enumerate(success_subtree):\n",
    "    print('\\r%d / %d' %(i, n), end='')\n",
    "    if len(d[\"tokens\"]) != 1:\n",
    "        success_subtree_at.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "97802ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_bin(success_subtree_at,\"temp/success_subtree_A-B.binaryfile\")\n",
    "# save_bin(fail_subtree_2,\"temp/fail_subtree_A-B.binaryfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297e62f",
   "metadata": {},
   "source": [
    "# ↓23/8/18\n",
    "【今後の予定】  \n",
    "・もとのsmallにはidがついていない＝＞3/167（ID付き）を新たに10作成　【18日完了】  \n",
    "・その文番号のみのsmall_subtreeを新たに作成 【18日完了】  \n",
    "・交換先の選定　【18日完了】  \n",
    "・交換時の比率を決定＝＞1文につき1か所  【18日完了】  \n",
    "・交換候補からROOTや.を削除 【19日完了】  \n",
    "・交換後の文生成    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99cfc95",
   "metadata": {},
   "source": [
    "### id付きの3/167データ×10を新たに準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e89e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = []\n",
    "# with open(\"data/f_id/train.json\",\"r\") as f:\n",
    "#     for line in f:\n",
    "#         train.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"art-broadcastprogram\",\"art-film\",\"art-music\",\"art-other\",\"art-painting\",\"art-writtenart\",\"building-airport\",\"building-hospital\",\"building-hotel\",\"building-library\",\"building-other\",\"building-restaurant\",\"building-sportsfacility\",\"building-theater\",\"event-attack/battle/war/militaryconflict\",\"event-disaster\",\"event-election\",\"event-other\",\"event-protest\",\"event-sportsevent\",\"location-GPE\",\"location-bodiesofwater\",\"location-island\",\"location-mountain\",\"location-other\",\"location-park\",\"location-road/railway/highway/transit\",\"organization-company\",\"organization-education\",\"organization-government/governmentagency\",\"organization-media/newspaper\",\"organization-other\",\"organization-politicalparty\",\"organization-religion\",\"organization-showorganization\",\"organization-sportsleague\",\"organization-sportsteam\",\"other-astronomything\",\"other-award\",\"other-biologything\",\"other-chemicalthing\",\"other-currency\",\"other-disease\",\"other-educationaldegree\",\"other-god\",\"other-language\",\"other-law\",\"other-livingthing\",\"other-medical\",\"person-actor\",\"person-artist/author\",\"person-athlete\",\"person-director\",\"person-other\",\"person-politician\",\"person-scholar\",\"person-soldier\",\"product-airplane\",\"product-car\",\"product-food\",\"product-game\",\"product-other\",\"product-ship\",\"product-software\",\"product-train\",\"product-weapon\",\"O\"]\n",
    "# 66種のラベルごとに対応するリスト ラベルを含むtrainの各要素を含むリストが入っている値\n",
    "labeled_data = []\n",
    "for label in labels:\n",
    "    new_l = []\n",
    "    for d in train:\n",
    "        if label == \"O\":\n",
    "            if {\"O\"} == set(d[\"tags\"]):\n",
    "                new_l.append(d)\n",
    "        else:\n",
    "            if label in set(d[\"tags\"]):\n",
    "                new_l.append(d)\n",
    "    labeled_data.append(new_l)#listに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b648014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3/167スケールのデータを新たに10作成（id付き）\n",
    "import random\n",
    "n = 3\n",
    "for ver in range(0,10):\n",
    "    labeled_data_elected = []#21548\n",
    "    for i in range(len(labeled_data)):\n",
    "        choice_num = int(len(labeled_data[i])*n/166)\n",
    "        print(choice_num,\"/\",len(labeled_data[i]),end = \"    \")\n",
    "        choice_list = random.sample(labeled_data[i],choice_num)\n",
    "        labeled_data_elected.extend(choice_list)\n",
    "    print(len(labeled_data_elected))\n",
    "\n",
    "    #f_smallを作成\n",
    "    with open(\"data/f_small_id_3-167/original/\"+str(ver)+\".json\", 'w') as f:\n",
    "        for d in labeled_data_elected:\n",
    "            f.write(json.dumps(d))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1400fe",
   "metadata": {},
   "source": [
    "# ↓23/8/19\n",
    "【今後の予定】  \n",
    "・きちんと部分一致するtoken群を探すべき【20日完了】\n",
    "→ tokenひとつひとつを検索した場合，一番最初の全く違うトークンを拾ってきてしまう可能性がある  \n",
    "・交換後の文生成【20日完了】"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0c3bc",
   "metadata": {},
   "source": [
    "### 新たに生成したデータの読み込み，DA加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfce4dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 深さとタグから候補部分木をリストで返す関数\n",
    "\n",
    "# 引数であるsentidにset又はidをとり，sentidに一致するidのsubreeだけを返す関数\n",
    "def search_d_sentid(sbts_list,q_id):# max(depth):34\n",
    "    found_sbts = []\n",
    "    for d in sbts_list:\n",
    "        if type(q_id) == set:\n",
    "            if d[\"id\"] in q_id:\n",
    "                found_sbts.append(d)\n",
    "        elif type(q_id) == int:\n",
    "            if d[\"id\"] == q_id:\n",
    "                found_sbts.append(d)\n",
    "    return found_sbts\n",
    "\n",
    "# 深さとタグから候補部分木をリストで返す関数\n",
    "def search_d_depth_and_tag(sbts_list,q_depth,q_tree_tag):# max(depth):34\n",
    "    found_sbts = []\n",
    "    for d in sbts_list:\n",
    "        if d[\"depth\"] == q_depth and d[\"tree_tag\"] == q_tree_tag:\n",
    "            found_sbts.append(d)\n",
    "    return found_sbts\n",
    "\n",
    "# タグのみから候補部分木をリストで返す関数\n",
    "def search_d_tag(sbts_list,q_tree_tag):# max(depth):34\n",
    "    found_sbts = []\n",
    "    for d in sbts_list:\n",
    "        if d[\"tree_tag\"] == q_tree_tag:\n",
    "            found_sbts.append(d)\n",
    "    return found_sbts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f019a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定部分木tokenを交換用部分木tokenに入れ替え，tagを対応させた辞書を返す関数\n",
    "def swap_tokens(f_d,rand_sbt_in_s,rand_exch_pair):\n",
    "    new_d = dict()\n",
    "    i = rand_sbt_in_s[\"start_index\"]\n",
    "    n = len(rand_sbt_in_s[\"tokens\"])\n",
    "    new_d[\"tokens\"] = f_d[\"tokens\"][:i]+rand_exch_pair[\"tokens\"]+f_d[\"tokens\"][i+n:]\n",
    "    new_d[\"tags\"] = f_d[\"tags\"][:i]+rand_exch_pair[\"tags\"]+f_d[\"tags\"][i+n:]\n",
    "#     new_d[\"ids\"] = f_d[\"id\"]\n",
    "    return new_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e008a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "subtrees = read_bin(\"temp/success_subtree_A-B.binaryfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3881199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9) 4191 / 4192"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "no_sbt_s = set()\n",
    "no_exc_sbt = set()\n",
    "\n",
    "for n in range(1,10):\n",
    "    train_f = []# 該当ファイルのjsonデータ\n",
    "    with open(\"data/f_small_id_3-167/original/\"+str(n)+\".json\", \"r\") as f:\n",
    "        for line in f:\n",
    "            train_f.append(json.loads(line))        \n",
    "    len_train = len(train_f)\n",
    "    ids_in_f = set([d[\"id\"] for d in train_f])\n",
    "    sbts_in_f = [d for d in subtrees if d[\"id\"] in ids_in_f]\n",
    "    \n",
    "    data_f_path = \"data/SubTree/\"+str(n)+\"_x10.json\"\n",
    "    log_f_path = \"log/SubTree_DA_log/\"+str(n)+\"_log.txt\"\n",
    "    \n",
    "    with open(data_f_path,\"w\",encoding='utf-8') as data_f, open(log_f_path,\"w\",encoding='utf-8') as log_f:\n",
    "        for i,d in enumerate(train_f):\n",
    "            print('\\r(%d) %d / %d' %(n,i,len_train), end='')\n",
    "\n",
    "            data_f.write(json.dumps({\"tokens\":d[\"tokens\"],\"tags\":d[\"tags\"]}))#データを含める\n",
    "            data_f.write(\"\\n\")        \n",
    "            log_f.write(\"★ \"+\" \".join(d[\"tokens\"])+\"\\n\")# 元データのログ\n",
    "\n",
    "            for _ in range(10):\n",
    "                sbts_in_s = search_d_sentid(sbts_in_f,d[\"id\"])# 部分木群\n",
    "                \n",
    "                if len(sbts_in_s) == 0:# 部分木の無い文を報告\n",
    "                    no_sbt_s.add(d[\"id\"])\n",
    "                    log_f.write(\" 　【no-subtree-ERROR】[id:\"+str(d[\"id\"])+\"]\\n\")# 交換のログ出力\n",
    "                    break\n",
    "                else:\n",
    "                    rand_sbt = random.choice(sbts_in_s)# ランダム部分木一つ\n",
    "                    \n",
    "                    exch_pairs = search_d_depth_and_tag(\n",
    "                        sbts_in_f,rand_sbt[\"depth\"],rand_sbt[\"tree_tag\"])# 交換候補群\n",
    "                    \n",
    "                    if len(exch_pairs) == 0:# 交換候補の無い部分木を報告\n",
    "                        no_exc_sbt.add(rand_sbt)\n",
    "                        log_f.write(\" 　【no-pair-ERROR】[\"+\" \".join(rand_sbt[\"tokens\"])+\"]\\n\")# 交換のログ出力\n",
    "                        break\n",
    "                    else:\n",
    "                        rand_exch_pair = random.choice(exch_pairs)# ランダム交換候補一つ\n",
    "                        \n",
    "                        log_f.write(\" 　・ \"+\" \".join(rand_sbt[\"tokens\"])+\n",
    "                        \" => \"+\" \".join(rand_exch_pair[\"tokens\"])+\"\\n\")# 交換のログ出力\n",
    "\n",
    "                        new_d = swap_tokens(d,rand_sbt,rand_exch_pair)\n",
    "                        data_f.write(json.dumps(new_d))\n",
    "                        data_f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6c241",
   "metadata": {},
   "source": [
    "# 23/8/20 & 21　\n",
    "\n",
    "【今後の予定】  \n",
    "・エラーで止まってしまったので対応【20日完了】  \n",
    "・エラーのチェック → A-Bのエラー修正【21日完了】 \n",
    "・データ完成 【21日完了】  \n",
    "・データをFlowに送り学習【22日完了】  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ee0be",
   "metadata": {},
   "source": [
    "# 23/9/5\n",
    "\n",
    "【本日のお品書き】  \n",
    "・otherの癒着問題を修正  \n",
    "・結果を未学習のトークンとし再集計  \n",
    "・結果出力用ＨＴＭＬファイルの作成  \n",
    "　・各固有表現タグの学習データ  \n",
    "　・各固有表現タグのテストデータ（正解したもの）  \n",
    "　・各固有表現タグのテストデータ（不正解だったもの）  \n",
    "　・誤って該当固有表現に分類されたテストデータ  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1dba3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'DA4ner' from 'C:\\\\Users\\\\Teramoto\\\\Desktop\\\\git\\\\NER-DA\\\\DA4ner.py'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import DA4ner\n",
    "import importlib\n",
    "importlib.reload(DA4ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d9257f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 / 9"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "result = []\n",
    "for i in range(10):\n",
    "    print('\\r%d / %d' %(i, 9), end='')\n",
    "    result.append(DA4ner.manual_NER_eval(\"data/f/test.json\",\"result/SubTree/\"+str(i)+\"/predictions.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d83a9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.io.json.json_normalize(result)\n",
    "df.to_csv('analysis/Subtree/manual_NRE_eval_0-9.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da79e03",
   "metadata": {},
   "source": [
    "# confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "750ed7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# エラー発生\n",
    "# import warnings\n",
    "# warnings.simplefilter('ignore')\n",
    "\n",
    "# for i in range(10):\n",
    "#     print('\\r%d / %d' %(i, 9), end='')\n",
    "#     DA4ner.manual_matrix(\"data/f/test.json\",\"result/SubTree/\"+str(i)+\"/predictions.txt\",\"analysis/Subtree/confusion_matrix_\"+str(i)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abe321a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "pre_s = {}\n",
    "ref = []\n",
    "tokens = []\n",
    "task_list = [\"f_small\",\"TreePos_All_small\",\"SubTree\"]\n",
    "\n",
    "with open(\"data/f/test.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        ref.append(json.loads(line)[\"tags\"])\n",
    "        tokens.append(json.loads(line)[\"tokens\"])\n",
    "pre_s[\"tokens\"] = tokens\n",
    "pre_s[\"ans\"] = ref\n",
    "\n",
    "for task_name in task_list:\n",
    "    pre = []\n",
    "    with open(\"analysis/FvsPOSALLvsSUBTREE/\"+task_name+\"/predictions.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            pre.append(line.replace(\"\\n\",\"\").split(\" \"))\n",
    "    pre_s[task_name] = pre\n",
    "    \n",
    "pre_lines = []\n",
    "for i in range(len(pre_s[\"ans\"])):\n",
    "    new_d = {}\n",
    "    for k in [\"tokens\",\"ans\"]+task_list:\n",
    "        new_d[k]=pre_s[k][i]\n",
    "    pre_lines.append(new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28d97edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3bb0d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "art-music\n",
      "location-road/railway/highway/transit\n",
      "product-software\n",
      "person-soldier\n",
      "art-film\n",
      "other-medical\n",
      "organization-education\n",
      "product-train\n",
      "other-currency\n",
      "other-educationaldegree\n",
      "organization-politicalparty\n",
      "event-sportsevent\n",
      "event-attack/battle/war/militaryconflict\n",
      "organization-sportsteam\n",
      "organization-religion\n",
      "person-other\n",
      "product-other\n",
      "location-bodiesofwater\n",
      "product-car\n",
      "building-other\n",
      "other-chemicalthing\n",
      "building-library\n",
      "building-theater\n",
      "event-protest\n",
      "organization-media/newspaper\n",
      "organization-government/governmentagency\n",
      "person-director\n",
      "other-language\n",
      "location-GPE\n",
      "art-other\n",
      "location-mountain\n",
      "organization-other\n",
      "building-restaurant\n",
      "other-astronomything\n",
      "other-award\n",
      "building-airport\n",
      "other-livingthing\n",
      "person-politician\n",
      "product-airplane\n",
      "art-broadcastprogram\n",
      "art-writtenart\n",
      "organization-company\n",
      "person-artist/author\n",
      "product-ship\n",
      "event-other\n",
      "other-biologything\n",
      "person-scholar\n",
      "event-disaster\n",
      "organization-showorganization\n",
      "location-park\n",
      "other-god\n",
      "art-painting\n",
      "location-island\n",
      "location-other\n",
      "other-disease\n",
      "person-actor\n",
      "organization-sportsleague\n",
      "product-game\n",
      "person-athlete\n",
      "building-hospital\n",
      "event-election\n",
      "building-hotel\n",
      "product-food\n",
      "building-sportsfacility\n",
      "other-law\n",
      "product-weapon\n"
     ]
    }
   ],
   "source": [
    "label_set = set()\n",
    "with open(\"data/labels.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        label_set.add(line.replace(\"\\n\",\"\"))\n",
    "label_set.remove(\"O\")\n",
    "\n",
    "for label in label_set:\n",
    "    print(label)\n",
    "    all_txt = \"\"\n",
    "    with open(\"analysis/SubTree_csv_html/html/\"+label.replace(\"/\",\"_\")+\".html\", mode='w',encoding='utf-8') as f:\n",
    "        for d in pre_lines:\n",
    "            ans = \"ans              :\"\n",
    "            small = \"small            :\"\n",
    "            TreePos_All_small = \"TreePos_ALL_small:\"\n",
    "            SubTree = \"SubTree          :\"\n",
    "            if label in d[\"ans\"]:\n",
    "                if d[\"ans\"]!=d[\"f_small\"] or d[\"ans\"]!=d[\"TreePos_All_small\"] or d[\"ans\"]!=d[\"SubTree\"]:\n",
    "                    for i, t in enumerate(d[\"tokens\"]):\n",
    "\n",
    "                        if d[\"ans\"][i] != \"O\":\n",
    "                            ans += \"<font color='blue'>\"+t+\"</font> \"\n",
    "                        else:\n",
    "                            ans += t+\" \"\n",
    "\n",
    "                        if d[\"f_small\"][i] != \"O\":\n",
    "                            small += \"<font color='blue'>\"+t+\"</font> \"\n",
    "                        else:\n",
    "                            small += t+\" \"\n",
    "\n",
    "                        if d[\"TreePos_All_small\"][i] != \"O\":\n",
    "                            TreePos_All_small += \"<font color='blue'>\"+t+\"</font> \"\n",
    "                        else:\n",
    "                            TreePos_All_small += t+\" \"\n",
    "\n",
    "                        if d[\"SubTree\"][i] != \"O\":\n",
    "                            SubTree += \"<font color='blue'>\"+t+\"</font> \"\n",
    "                        else:\n",
    "                            SubTree += t+\" \"\n",
    "                    f.write(\" \".join(d[\"ans\"])+\"<br>\"+\" \".join(d[\"f_small\"])+\"<br>\"+\" \".join(d[\"TreePos_All_small\"])+\"<br>\"+\" \".join(d[\"SubTree\"])+\"<br><br>\")\n",
    "                    f.write(ans+\"<br>\"+small+\"<br>\"+TreePos_All_small+\"<br>\"+SubTree+\"<br><br>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c62426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
